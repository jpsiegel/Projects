{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tarea2-DL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOWYfice26VuWlnCZMLk32E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jpsiegel/Projects/blob/master/Tarea2_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qa3fduSBx5NE"
      },
      "source": [
        "#Tarea 2 Jan P. Siegel - Deep Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mspk4ICBxny2",
        "outputId": "7dbc7732-6061-42cb-932c-92e54fa35c28"
      },
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp37-none-any.whl size=7411 sha256=bbfcdf5e7aecab7e94ddfbb88233e29eb494bad7d4d5f77ba3db5c2555be7c37\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.7/dist-packages (0.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2063EAySx_qJ",
        "outputId": "e68d40e3-1d74-4dc5-d623-654a7a5ac2c9"
      },
      "source": [
        "GPUs = GPU.getGPUs()\n",
        "gpu = GPUs[0]  # Only one GPU on Colab and not guaranteed\n",
        "\n",
        "def printm():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(\"RAM Free: \" + humanize.naturalsize(psutil.virtual_memory().available), \" | Used: \" + humanize.naturalsize(process.memory_info().rss))\n",
        "  print(\"VRAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "\n",
        "printm()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RAM Free: 12.8 GB  | Used: 118.5 MB\n",
            "VRAM Free: 15109MB | Used: 0MB | Util   0% Total 15109MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xZ2AEIqG00P"
      },
      "source": [
        "##Parte 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZWm4FOUebL2"
      },
      "source": [
        "###Actividad 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUSA5oZ54TlW",
        "outputId": "149479ba-d2d7-4155-8e62-9044482c901e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "sms_data_path = '/content/drive/MyDrive/DL/T2/SMSSpamCollection'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_sEJcPW_A_-"
      },
      "source": [
        "!pip install d2l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KLHINtAN9qM",
        "outputId": "59f27bf8-cf1e-4756-bcee-6d743d794141"
      },
      "source": [
        "from string import punctuation\n",
        "from d2l import torch as d2l\n",
        "\n",
        "#  Reads sms data file\n",
        "raw_data = open(sms_data_path, \"r\")\n",
        "strings = raw_data.readlines()\n",
        "\n",
        "vocab_freq = {} # word : count\n",
        "\n",
        "processed_data = []\n",
        "processed_spam = []\n",
        "processed_nospam = []\n",
        "for s in strings:\n",
        "  s = s.strip(\"\\n\")\n",
        "  listed = s.split(\"\\t\")\n",
        "  label = 1 if listed[0] == \"spam\" else 0 # 1 is spam and 0 is not spam\n",
        "  phrase = listed[1].lower()\n",
        "  phrase = \"\".join([c for c in phrase if c not in punctuation]) # eliminate punctuation\n",
        "  words = phrase.split(\" \")\n",
        "  for word in words:\n",
        "    try:\n",
        "      vocab_freq[word] += 1\n",
        "    except KeyError:\n",
        "      vocab_freq[word] = 1\n",
        "  processed_data.append((label, words))\n",
        "  if label == 1:\n",
        "    processed_spam.append((label, words))\n",
        "  else:\n",
        "    processed_nospam.append((label, words))\n",
        "\n",
        "vocab = vocab_freq.keys()\n",
        "print(processed_data[:3]) # (label, lists_of_words)\n",
        "print(\"Vocab length:\", len(vocab)) # vocabulary of 9662 words\n",
        "print(\"Spamm data:\", len(processed_spam))\n",
        "print(\"No Spamm data:\", len(processed_nospam))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, ['go', 'until', 'jurong', 'point', 'crazy', 'available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'there', 'got', 'amore', 'wat']), (0, ['ok', 'lar', 'joking', 'wif', 'u', 'oni']), (1, ['free', 'entry', 'in', '2', 'a', 'wkly', 'comp', 'to', 'win', 'fa', 'cup', 'final', 'tkts', '21st', 'may', '2005', 'text', 'fa', 'to', '87121', 'to', 'receive', 'entry', 'questionstd', 'txt', 'ratetcs', 'apply', '08452810075over18s'])]\n",
            "Vocab length: 9662\n",
            "Spamm data: 747\n",
            "No Spamm data: 4827\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jArACI_rQWlU",
        "outputId": "728c9036-3576-49e1-cf19-b0c4e514a733"
      },
      "source": [
        "embedding_dim = \"100\" # dimensiones aceptadas 50, 100, 200, 300 (act 2 y 5)\n",
        "glove_embedding = d2l.TokenEmbedding(f'glove.6b.{embedding_dim}d') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading ../data/glove.6B.100d.zip from http://d2l-data.s3-accelerate.amazonaws.com/glove.6B.100d.zip...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pILAgK_UTok",
        "outputId": "4a1a91e8-6bc7-4a4c-8757-32df4d8bec5e"
      },
      "source": [
        "embeds = glove_embedding[vocab] # generate embedding for every word in vocabulary \n",
        "print(embeds.shape) # we have 9662 vectors of 100 dims"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([9662, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRl0lV4GejSO"
      },
      "source": [
        "La idea de implementar un embedding para las palabras es poder pasar de datos en texto a tensores vectoriales numéricos, esto permite expresar las palabras en un formato que les añada riqueza en su descripción semántica (haciendo que palabras similares tengan valores cercanos) y además las haga consumibles por un modelo de aprendizaje automático.\n",
        "\n",
        "En particular, GloVe no captura solamente las características estadísticas locales como Word2Vec, sino también las (Glo)bales para crear el (Ve)ctor deseado. Para lograr esto,  toma en cuenta relaciones semánticas locales, pero también deriva relaciones semánticas a partir de una matriz de co-ocurrencia, donde se indica qué tan probable es que una palabra aparezca en el contexto de otra. La gracia de GloVe es que puede expresar esta relación en un vector limpiamente con escasa pérdida de información."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLOcvDNreUb2"
      },
      "source": [
        "###Actividad 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_1Dc6rS9Llv"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class RNN(nn.Module):\n",
        "  \n",
        "  def __init__(self, input_dim, hidden_dim, num_layers):\n",
        "    super().__init__() #input_dim es la dimension del x_t, es la dimension que te deja el embedding (ej word2vec)\n",
        "    self.hidden_dim = hidden_dim # dimensiones de h_t\n",
        "    self.num_layers = num_layers # cantidad de capas (filas) ocultas\n",
        "    self.rnn = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "    self.linear_out = nn.Linear(hidden_dim, 1) # creo que esto es y\n",
        "    \n",
        "  # This method defines the forward pass of the RNN\n",
        "  def forward(self, input):\n",
        "    batch_size, _ = input.size()\n",
        "\n",
        "    # Initializing hidden state for first input\n",
        "    h0 = self.init_hidden(batch_size)\n",
        "    # Passing in the input and hidden state to obtain output\n",
        "    _, hidden_state = self.rnn(input.unsqueeze(2), h0)\n",
        "    out = self.linear_out(hidden_state.squeeze())\n",
        "    return out\n",
        "    \n",
        "  # This method generates the first hidden state of zeros for the forward pass\n",
        "  # This creates a tensor of zeros in the shape of our hidden states.\n",
        "  def init_hidden(self, batch_size):\n",
        "    hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim)\n",
        "    return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUVwQo6eCcC6"
      },
      "source": [
        "##Referencias\n",
        "https://www.kaggle.com/anindya2906/glove6b"
      ]
    }
  ]
}